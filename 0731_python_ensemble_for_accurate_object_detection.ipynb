{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCA08MxVOlkFoyhDMiymJt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex-Jung-HB/0731_python_ensemble-for-accurate-object-detection/blob/main/0731_python_ensemble_for_accurate_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemable using the most practical method of Weighted Boxes Fusion(WBF)"
      ],
      "metadata": {
        "id": "qmJBsf8zV3lX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dUn5KZ3DSVDd",
        "outputId": "a88bafda-fe45-41ae-e0b2-0e59d1579c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.170-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting supervision\n",
            "  Downloading supervision-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting ensemble-boxes\n",
            "  Downloading ensemble_boxes-1.0.9-py3-none-any.whl.metadata (728 bytes)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.16.0)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from ensemble-boxes) (0.60.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->ensemble-boxes) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.170-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading supervision-0.26.1-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ensemble_boxes-1.0.9-py3-none-any.whl (23 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, supervision, nvidia-cusolver-cu12, ensemble-boxes, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed ensemble-boxes-1.0.9 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 supervision-0.26.1 ultralytics-8.3.170 ultralytics-thop-2.0.14\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\n",
            "üìä ENSEMBLE DETECTION SYSTEM SUMMARY\n",
            "----------------------------------------\n",
            "üéØ Target Classes for Autonomous Driving:\n",
            "   ‚Ä¢ Vehicles: car, truck, bus, motorcycle\n",
            "   ‚Ä¢ Pedestrians: person\n",
            "   ‚Ä¢ Cyclists: bicycle\n",
            "   ‚Ä¢ Infrastructure: traffic_light\n",
            "\n",
            "üß† Ensemble Architecture:\n",
            "   ‚Ä¢ YOLOv8n: Real-time detection (Weight: 1.0)\n",
            "   ‚Ä¢ YOLOv8s: Balanced performance (Weight: 1.2)\n",
            "   ‚Ä¢ YOLOv8m: High accuracy (Weight: 1.5)\n",
            "\n",
            "‚ö° Fusion Method:\n",
            "   ‚Ä¢ Weighted Boxes Fusion (WBF)\n",
            "   ‚Ä¢ IoU Threshold: 0.6\n",
            "   ‚Ä¢ Confidence Threshold: 0.01\n",
            "============================================================\n",
            "üöó ENSEMBLE OBJECT DETECTION FOR AUTONOMOUS DRIVING\n",
            "============================================================\n",
            "üîÑ Loading ensemble models for autonomous driving...\n",
            "üì¶ Loading YOLOv8n (Speed optimized)...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.25M/6.25M [00:00<00:00, 104MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Loading YOLOv8s (Balanced performance)...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.5M/21.5M [00:00<00:00, 207MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Loading YOLOv8m (Accuracy optimized)...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49.7M/49.7M [00:00<00:00, 155MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully loaded 3 models for ensemble\n",
            "üìπ Choose option:\n",
            "1. Upload your video\n",
            "2. Use sample video\n",
            "Enter choice (1 or 2): 1\n",
            "üì§ Please upload your driving video file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c0903f3-dfe6-4f8e-87b0-7cb67bef4c08\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1c0903f3-dfe6-4f8e-87b0-7cb67bef4c08\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# =====================================================\n",
        "# ENSEMBLE OBJECT DETECTION FOR AUTONOMOUS DRIVING\n",
        "# Google Colab Compatible Version\n",
        "# =====================================================\n",
        "\n",
        "# Install required dependencies\n",
        "!pip install ultralytics supervision ensemble-boxes opencv-python-headless\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "import ensemble_boxes\n",
        "from google.colab import files\n",
        "import os\n",
        "import zipfile\n",
        "from IPython.display import Video, display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# =====================================================\n",
        "# ENSEMBLE DETECTION CLASSES AND CONFIGURATION\n",
        "# =====================================================\n",
        "\n",
        "class AutonomousDrivingEnsemble:\n",
        "    \"\"\"\n",
        "    Ensemble Object Detection System for Autonomous Driving\n",
        "    Combines multiple YOLO models using Weighted Boxes Fusion (WBF)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Detection classes relevant for autonomous driving\n",
        "        self.AD_CLASSES = {\n",
        "            0: 'person',      # Pedestrians\n",
        "            1: 'bicycle',     # Bicycles\n",
        "            2: 'car',         # Cars\n",
        "            3: 'motorcycle',  # Motorcycles\n",
        "            5: 'bus',         # Buses\n",
        "            7: 'truck',       # Trucks\n",
        "            9: 'traffic_light', # Traffic lights\n",
        "            # Additional custom classes can be added\n",
        "        }\n",
        "\n",
        "        # Color mapping for visualization\n",
        "        self.COLORS = {\n",
        "            'person': (0, 255, 255),      # Yellow\n",
        "            'bicycle': (255, 0, 255),     # Magenta\n",
        "            'car': (0, 0, 255),           # Red\n",
        "            'motorcycle': (255, 255, 0),  # Cyan\n",
        "            'bus': (0, 255, 0),           # Green\n",
        "            'truck': (255, 0, 0),         # Blue\n",
        "            'traffic_light': (128, 255, 0), # Light Green\n",
        "        }\n",
        "\n",
        "        # Detection statistics\n",
        "        self.stats = {class_name: 0 for class_name in self.AD_CLASSES.values()}\n",
        "\n",
        "        # Initialize models\n",
        "        self.models = []\n",
        "        self.model_weights = []\n",
        "\n",
        "    def load_ensemble_models(self):\n",
        "        \"\"\"\n",
        "        Load multiple YOLO models for ensemble detection\n",
        "        Each model brings different strengths and perspectives\n",
        "        \"\"\"\n",
        "        print(\"üîÑ Loading ensemble models for autonomous driving...\")\n",
        "\n",
        "        try:\n",
        "            # Model 1: YOLOv8n - Fast detection for real-time performance\n",
        "            print(\"üì¶ Loading YOLOv8n (Speed optimized)...\")\n",
        "            model1 = YOLO('yolov8n.pt')\n",
        "            self.models.append(model1)\n",
        "            self.model_weights.append(1.0)  # Equal weight\n",
        "\n",
        "            # Model 2: YOLOv8s - Balanced speed and accuracy\n",
        "            print(\"üì¶ Loading YOLOv8s (Balanced performance)...\")\n",
        "            model2 = YOLO('yolov8s.pt')\n",
        "            self.models.append(model2)\n",
        "            self.model_weights.append(1.2)  # Slightly higher weight\n",
        "\n",
        "            # Model 3: YOLOv8m - Higher accuracy for critical detections\n",
        "            print(\"üì¶ Loading YOLOv8m (Accuracy optimized)...\")\n",
        "            model3 = YOLO('yolov8m.pt')\n",
        "            self.models.append(model3)\n",
        "            self.model_weights.append(1.5)  # Highest weight\n",
        "\n",
        "            print(f\"‚úÖ Successfully loaded {len(self.models)} models for ensemble\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading models: {e}\")\n",
        "            # Fallback to single model\n",
        "            print(\"üîÑ Loading fallback single model...\")\n",
        "            model = YOLO('yolov8n.pt')\n",
        "            self.models = [model]\n",
        "            self.model_weights = [1.0]\n",
        "\n",
        "    def weighted_boxes_fusion(self, predictions_list, image_size, iou_threshold=0.6, skip_box_threshold=0.01):\n",
        "        \"\"\"\n",
        "        Implement Weighted Boxes Fusion (WBF) algorithm\n",
        "        Combines predictions from multiple models intelligently\n",
        "\n",
        "        Args:\n",
        "            predictions_list: List of predictions from different models\n",
        "            image_size: (width, height) of the image\n",
        "            iou_threshold: IoU threshold for box clustering\n",
        "            skip_box_threshold: Confidence threshold for filtering boxes\n",
        "\n",
        "        Returns:\n",
        "            Fused detection results\n",
        "        \"\"\"\n",
        "        if not predictions_list:\n",
        "            return [], [], []\n",
        "\n",
        "        # Prepare data for ensemble-boxes WBF\n",
        "        boxes_list = []\n",
        "        scores_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        img_width, img_height = image_size\n",
        "\n",
        "        # Convert each model's predictions to normalized format\n",
        "        for predictions in predictions_list:\n",
        "            if predictions is None or len(predictions) == 0:\n",
        "                # Empty predictions from this model\n",
        "                boxes_list.append([])\n",
        "                scores_list.append([])\n",
        "                labels_list.append([])\n",
        "                continue\n",
        "\n",
        "            boxes = []\n",
        "            scores = []\n",
        "            labels = []\n",
        "\n",
        "            # Extract detection data\n",
        "            for detection in predictions:\n",
        "                # Get bounding box coordinates (normalized to 0-1)\n",
        "                x1, y1, x2, y2 = detection[:4]\n",
        "                x1_norm = x1 / img_width\n",
        "                y1_norm = y1 / img_height\n",
        "                x2_norm = x2 / img_width\n",
        "                y2_norm = y2 / img_height\n",
        "\n",
        "                # Get confidence and class\n",
        "                confidence = float(detection[4])\n",
        "                class_id = int(detection[5])\n",
        "\n",
        "                # Filter by confidence and relevant classes\n",
        "                if confidence > skip_box_threshold and class_id in self.AD_CLASSES:\n",
        "                    boxes.append([x1_norm, y1_norm, x2_norm, y2_norm])\n",
        "                    scores.append(confidence)\n",
        "                    labels.append(class_id)\n",
        "\n",
        "            boxes_list.append(boxes)\n",
        "            scores_list.append(scores)\n",
        "            labels_list.append(labels)\n",
        "\n",
        "        # Apply Weighted Boxes Fusion\n",
        "        try:\n",
        "            fused_boxes, fused_scores, fused_labels = ensemble_boxes.weighted_boxes_fusion(\n",
        "                boxes_list,\n",
        "                scores_list,\n",
        "                labels_list,\n",
        "                weights=self.model_weights,\n",
        "                iou_thr=iou_threshold,\n",
        "                skip_box_thr=skip_box_threshold\n",
        "            )\n",
        "\n",
        "            # Convert back to pixel coordinates\n",
        "            final_boxes = []\n",
        "            for box in fused_boxes:\n",
        "                x1 = int(box[0] * img_width)\n",
        "                y1 = int(box[1] * img_height)\n",
        "                x2 = int(box[2] * img_width)\n",
        "                y2 = int(box[3] * img_height)\n",
        "                final_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "            return final_boxes, fused_scores, fused_labels\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è WBF fusion failed: {e}. Using first model's predictions.\")\n",
        "            # Fallback to first model's predictions\n",
        "            if predictions_list and len(predictions_list[0]) > 0:\n",
        "                pred = predictions_list[0]\n",
        "                boxes = [[int(x) for x in det[:4]] for det in pred if det[4] > skip_box_threshold]\n",
        "                scores = [float(det[4]) for det in pred if det[4] > skip_box_threshold]\n",
        "                labels = [int(det[5]) for det in pred if det[4] > skip_box_threshold]\n",
        "                return boxes, scores, labels\n",
        "            return [], [], []\n",
        "\n",
        "    def detect_frame(self, frame):\n",
        "        \"\"\"\n",
        "        Run ensemble detection on a single frame\n",
        "\n",
        "        Args:\n",
        "            frame: Input image frame\n",
        "\n",
        "        Returns:\n",
        "            Annotated frame with detections\n",
        "        \"\"\"\n",
        "        h, w = frame.shape[:2]\n",
        "        predictions_list = []\n",
        "\n",
        "        # Get predictions from each model in the ensemble\n",
        "        for i, model in enumerate(self.models):\n",
        "            try:\n",
        "                # Run inference\n",
        "                results = model(frame, verbose=False)\n",
        "\n",
        "                # Extract detections\n",
        "                if results and len(results) > 0 and results[0].boxes is not None:\n",
        "                    boxes = results[0].boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2\n",
        "                    confidences = results[0].boxes.conf.cpu().numpy()\n",
        "                    class_ids = results[0].boxes.cls.cpu().numpy()\n",
        "\n",
        "                    # Combine into detection format\n",
        "                    detections = []\n",
        "                    for j in range(len(boxes)):\n",
        "                        detection = [\n",
        "                            boxes[j][0], boxes[j][1], boxes[j][2], boxes[j][3],  # bbox\n",
        "                            confidences[j],  # confidence\n",
        "                            class_ids[j]     # class_id\n",
        "                        ]\n",
        "                        detections.append(detection)\n",
        "\n",
        "                    predictions_list.append(detections)\n",
        "                else:\n",
        "                    predictions_list.append([])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Model {i} inference failed: {e}\")\n",
        "                predictions_list.append([])\n",
        "\n",
        "        # Apply Weighted Boxes Fusion\n",
        "        fused_boxes, fused_scores, fused_labels = self.weighted_boxes_fusion(\n",
        "            predictions_list, (w, h)\n",
        "        )\n",
        "\n",
        "        # Draw detections on frame\n",
        "        annotated_frame = self.draw_detections(frame.copy(), fused_boxes, fused_scores, fused_labels)\n",
        "\n",
        "        # Update statistics\n",
        "        self.update_stats(fused_labels)\n",
        "\n",
        "        return annotated_frame\n",
        "\n",
        "    def draw_detections(self, frame, boxes, scores, labels):\n",
        "        \"\"\"\n",
        "        Draw detection bounding boxes and labels on frame\n",
        "        \"\"\"\n",
        "        for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n",
        "            if int(label) not in self.AD_CLASSES:\n",
        "                continue\n",
        "\n",
        "            class_name = self.AD_CLASSES[int(label)]\n",
        "            color = self.COLORS.get(class_name, (255, 255, 255))\n",
        "\n",
        "            # Draw bounding box\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "            # Prepare label text\n",
        "            label_text = f\"{class_name}: {score:.2f}\"\n",
        "\n",
        "            # Draw label background\n",
        "            (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "            cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)\n",
        "\n",
        "            # Draw label text\n",
        "            cv2.putText(frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "            # Draw model ensemble indicator (small circle)\n",
        "            cv2.circle(frame, (x2 - 10, y1 + 10), 5, (0, 255, 0), -1)\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def update_stats(self, labels):\n",
        "        \"\"\"Update detection statistics\"\"\"\n",
        "        for label in labels:\n",
        "            if int(label) in self.AD_CLASSES:\n",
        "                class_name = self.AD_CLASSES[int(label)]\n",
        "                self.stats[class_name] += 1\n",
        "\n",
        "    def process_video(self, input_path, output_path):\n",
        "        \"\"\"\n",
        "        Process entire video with ensemble detection\n",
        "\n",
        "        Args:\n",
        "            input_path: Path to input video file\n",
        "            output_path: Path for output video file\n",
        "        \"\"\"\n",
        "        print(f\"üé¨ Processing video: {input_path}\")\n",
        "\n",
        "        # Open video capture\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        if not cap.isOpened():\n",
        "            raise Exception(f\"‚ùå Cannot open video file: {input_path}\")\n",
        "\n",
        "        # Get video properties\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        print(f\"üìä Video Info: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
        "\n",
        "        # Setup video writer\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "        # Reset statistics\n",
        "        self.stats = {class_name: 0 for class_name in self.AD_CLASSES.values()}\n",
        "\n",
        "        frame_count = 0\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Process frame with ensemble detection\n",
        "                processed_frame = self.detect_frame(frame)\n",
        "\n",
        "                # Add frame info overlay\n",
        "                info_text = f\"Frame: {frame_count}/{total_frames} | Ensemble: {len(self.models)} models\"\n",
        "                cv2.putText(processed_frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "                # Write processed frame\n",
        "                out.write(processed_frame)\n",
        "\n",
        "                frame_count += 1\n",
        "\n",
        "                # Progress indicator\n",
        "                if frame_count % 30 == 0:  # Every 30 frames\n",
        "                    progress = (frame_count / total_frames) * 100\n",
        "                    print(f\"üîÑ Progress: {progress:.1f}% ({frame_count}/{total_frames})\")\n",
        "\n",
        "        finally:\n",
        "            # Cleanup\n",
        "            cap.release()\n",
        "            out.release()\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "        print(f\"‚úÖ Video processing complete: {output_path}\")\n",
        "        print(\"üìà Detection Statistics:\")\n",
        "        for class_name, count in self.stats.items():\n",
        "            if count > 0:\n",
        "                print(f\"   {class_name}: {count}\")\n",
        "\n",
        "# =====================================================\n",
        "# MAIN EXECUTION FUNCTIONS\n",
        "# =====================================================\n",
        "\n",
        "def upload_video():\n",
        "    \"\"\"Upload video file to Colab\"\"\"\n",
        "    print(\"üì§ Please upload your driving video file...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        print(f\"‚úÖ Uploaded: {filename}\")\n",
        "        return filename\n",
        "    else:\n",
        "        print(\"‚ùå No file uploaded\")\n",
        "        return None\n",
        "\n",
        "def create_sample_video():\n",
        "    \"\"\"Create a sample video for testing (optional)\"\"\"\n",
        "    print(\"üé• Creating sample video for testing...\")\n",
        "\n",
        "    # Create a simple test video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter('sample_driving.mp4', fourcc, 20.0, (640, 480))\n",
        "\n",
        "    for i in range(100):  # 5 seconds at 20 FPS\n",
        "        # Create a simple frame with moving objects\n",
        "        frame = np.random.randint(0, 100, (480, 640, 3), dtype=np.uint8)\n",
        "\n",
        "        # Add some simple shapes to simulate objects\n",
        "        cv2.rectangle(frame, (50 + i*2, 200), (100 + i*2, 250), (0, 255, 0), -1)  # Moving car\n",
        "        cv2.circle(frame, (300, 300), 20, (255, 0, 0), -1)  # Traffic light\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    out.release()\n",
        "    print(\"‚úÖ Sample video created: sample_driving.mp4\")\n",
        "    return 'sample_driving.mp4'\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöó ENSEMBLE OBJECT DETECTION FOR AUTONOMOUS DRIVING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize ensemble system\n",
        "    ensemble = AutonomousDrivingEnsemble()\n",
        "\n",
        "    # Load ensemble models\n",
        "    ensemble.load_ensemble_models()\n",
        "\n",
        "    # Get input video\n",
        "    choice = input(\"üìπ Choose option:\\n1. Upload your video\\n2. Use sample video\\nEnter choice (1 or 2): \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        input_video = upload_video()\n",
        "        if not input_video:\n",
        "            print(\"‚ùå No video provided. Exiting.\")\n",
        "            return\n",
        "    else:\n",
        "        input_video = create_sample_video()\n",
        "\n",
        "    # Set output filename\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_video = f\"autonomous_driving_detected_{timestamp}.mp4\"\n",
        "\n",
        "    try:\n",
        "        # Process video with ensemble detection\n",
        "        ensemble.process_video(input_video, output_video)\n",
        "\n",
        "        # Create results summary\n",
        "        results_summary = {\n",
        "            'input_video': input_video,\n",
        "            'output_video': output_video,\n",
        "            'models_used': len(ensemble.models),\n",
        "            'detection_stats': ensemble.stats,\n",
        "            'processing_timestamp': timestamp\n",
        "        }\n",
        "\n",
        "        # Save results summary\n",
        "        summary_file = f\"detection_summary_{timestamp}.json\"\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(results_summary, f, indent=2)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"üéâ PROCESSING COMPLETE!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"üìÅ Output video: {output_video}\")\n",
        "        print(f\"üìä Summary file: {summary_file}\")\n",
        "\n",
        "        # Display video preview\n",
        "        if os.path.exists(output_video):\n",
        "            display(Video(output_video, width=600))\n",
        "\n",
        "        # Download files\n",
        "        print(\"\\nüì• Downloading processed files...\")\n",
        "        files.download(output_video)\n",
        "        files.download(summary_file)\n",
        "\n",
        "        # Create and download zip archive\n",
        "        zip_filename = f\"autonomous_driving_results_{timestamp}.zip\"\n",
        "        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "            zipf.write(output_video)\n",
        "            zipf.write(summary_file)\n",
        "\n",
        "        print(f\"üì¶ Created archive: {zip_filename}\")\n",
        "        files.download(zip_filename)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# =====================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# =====================================================\n",
        "\n",
        "def show_detection_summary():\n",
        "    \"\"\"Display detection performance summary\"\"\"\n",
        "    print(\"\\nüìä ENSEMBLE DETECTION SYSTEM SUMMARY\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"üéØ Target Classes for Autonomous Driving:\")\n",
        "    print(\"   ‚Ä¢ Vehicles: car, truck, bus, motorcycle\")\n",
        "    print(\"   ‚Ä¢ Pedestrians: person\")\n",
        "    print(\"   ‚Ä¢ Cyclists: bicycle\")\n",
        "    print(\"   ‚Ä¢ Infrastructure: traffic_light\")\n",
        "    print(\"\\nüß† Ensemble Architecture:\")\n",
        "    print(\"   ‚Ä¢ YOLOv8n: Real-time detection (Weight: 1.0)\")\n",
        "    print(\"   ‚Ä¢ YOLOv8s: Balanced performance (Weight: 1.2)\")\n",
        "    print(\"   ‚Ä¢ YOLOv8m: High accuracy (Weight: 1.5)\")\n",
        "    print(\"\\n‚ö° Fusion Method:\")\n",
        "    print(\"   ‚Ä¢ Weighted Boxes Fusion (WBF)\")\n",
        "    print(\"   ‚Ä¢ IoU Threshold: 0.6\")\n",
        "    print(\"   ‚Ä¢ Confidence Threshold: 0.01\")\n",
        "\n",
        "# =====================================================\n",
        "# EXECUTION\n",
        "# =====================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Show system information\n",
        "    show_detection_summary()\n",
        "\n",
        "    # Run main detection pipeline\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5RABEcmiXkws"
      }
    }
  ]
}